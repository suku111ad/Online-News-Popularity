# -*- coding: utf-8 -*-
"""Online News Popularity

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kJExI5TlhpZBZqNkkQfTTtQKV9VcxkB8

#**Predict popularity of Online News posts**

Import necessary packages
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io
import tensorflow as tf
from tensorflow.python.framework import ops
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import random
from random import sample
from google.colab import files
import statistics
from sklearn.preprocessing import StandardScaler
import math
from sklearn.metrics import log_loss
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score

"""#Our Dataset

Dataset Link: https://archive.ics.uci.edu/ml/datasets/online+news+popularity
"""

# Loading the data
uploaded = files.upload()

data = pd.read_csv(io.BytesIO(uploaded['OnlineNewsPopularity.csv']),encoding = 'unicode_escape')

data.head()

"""Dataset description

Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field) 

Attribute Information: 
0. url: URL of the article (non-predictive) 
1. timedelta: Days between the article publication and the dataset acquisition (non-predictive) 
2. n_tokens_title: Number of words in the title 
3. n_tokens_content: Number of words in the content 
4. n_unique_tokens: Rate of unique words in the content 
5. n_non_stop_words: Rate of non-stop words in the content 
6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content 
7. num_hrefs: Number of links 
8. num_self_hrefs: Number of links to other articles published by Mashable 
9. num_imgs: Number of images 
10. num_videos: Number of videos 
11. average_token_length: Average length of the words in the content 
12. num_keywords: Number of keywords in the metadata 
13. data_channel_is_lifestyle: Is data channel 'Lifestyle'? 
14. data_channel_is_entertainment: Is data channel 'Entertainment'? 
15. data_channel_is_bus: Is data channel 'Business'? 
16. data_channel_is_socmed: Is data channel 'Social Media'? 
17. data_channel_is_tech: Is data channel 'Tech'? 
18. data_channel_is_world: Is data channel 'World'? 
19. kw_min_min: Worst keyword (min. shares) 
20. kw_max_min: Worst keyword (max. shares) 
21. kw_avg_min: Worst keyword (avg. shares) 
22. kw_min_max: Best keyword (min. shares) 
23. kw_max_max: Best keyword (max. shares) 
24. kw_avg_max: Best keyword (avg. shares) 
25. kw_min_avg: Avg. keyword (min. shares) 
26. kw_max_avg: Avg. keyword (max. shares) 
27. kw_avg_avg: Avg. keyword (avg. shares) 
28. self_reference_min_shares: Min. shares of referenced articles in Mashable 
29. self_reference_max_shares: Max. shares of referenced articles in Mashable 
30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable 
31. weekday_is_monday: Was the article published on a Monday? 
32. weekday_is_tuesday: Was the article published on a Tuesday? 
33. weekday_is_wednesday: Was the article published on a Wednesday? 
34. weekday_is_thursday: Was the article published on a Thursday? 
35. weekday_is_friday: Was the article published on a Friday? 
36. weekday_is_saturday: Was the article published on a Saturday? 
37. weekday_is_sunday: Was the article published on a Sunday? 
38. is_weekend: Was the article published on the weekend? 
39. LDA_00: Closeness to LDA topic 0 
40. LDA_01: Closeness to LDA topic 1 
41. LDA_02: Closeness to LDA topic 2 
42. LDA_03: Closeness to LDA topic 3 
43. LDA_04: Closeness to LDA topic 4 
44. global_subjectivity: Text subjectivity 
45. global_sentiment_polarity: Text sentiment polarity 
46. global_rate_positive_words: Rate of positive words in the content 
47. global_rate_negative_words: Rate of negative words in the content 
48. rate_positive_words: Rate of positive words among non-neutral tokens 
49. rate_negative_words: Rate of negative words among non-neutral tokens 
50. avg_positive_polarity: Avg. polarity of positive words 
51. min_positive_polarity: Min. polarity of positive words 
52. max_positive_polarity: Max. polarity of positive words 
53. avg_negative_polarity: Avg. polarity of negative words 
54. min_negative_polarity: Min. polarity of negative words 
55. max_negative_polarity: Max. polarity of negative words 
56. title_subjectivity: Title subjectivity 
57. title_sentiment_polarity: Title polarity 
58. abs_title_subjectivity: Absolute subjectivity level 
59. abs_title_sentiment_polarity: Absolute polarity level 
60. shares: Number of shares (target)

#Exploratory Data Analysis

###Check for NULL values
"""

data.isnull().sum()

"""We do not have any null values in the data"""

data.describe()

"""###Shares vs Days of the Week"""

weekday= [' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',
       ' weekday_is_sunday']
weekd=data[weekday]

dic = {' weekday_is_monday' : [] , ' weekday_is_tuesday': [], ' weekday_is_wednesday' : [], ' weekday_is_thursday' : [], ' weekday_is_friday' : [], ' weekday_is_saturday' : [], ' weekday_is_sunday' : []}

length = len(weekd)

for day in weekday:
  for i in range(length):
      element = data[day][i]
      #print(element , type(element))
      if element == 1:
        dic[day].append(data[' shares'][i])

median = []
for day in weekday:
  med = statistics.median(dic[day])
  median.append(int(med))

print(median)

ax = sns.barplot(x= ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'], y= median)
ax.set(xlabel='day of week', ylabel='median_shares')
plt.show()
#data[' weekday_is_monday']

"""We can see articles published on Saturday and Sunday has got highest number of shares.

###Shares vs Type of Data Channel
"""

channel_list = [' data_channel_is_lifestyle',
       ' data_channel_is_entertainment', ' data_channel_is_bus',
       ' data_channel_is_socmed', ' data_channel_is_tech',
       ' data_channel_is_world']

dic = {' data_channel_is_lifestyle' : [],
       ' data_channel_is_entertainment':[], ' data_channel_is_bus':[],
       ' data_channel_is_socmed':[], ' data_channel_is_tech':[],
       ' data_channel_is_world':[]}       


for channel in channel_list:
  for i in range(length):
      element = data[channel][i]
      #print(element , type(element))
      if element == 1:
        dic[channel].append(data[' shares'][i])

median = []
for channel in channel_list:
  med = statistics.median(dic[channel])
  median.append(int(med))

print(median)

ax = sns.barplot(x=['lifestyle',
       'entertainment', 'bus',
       'socmed', 'tech',
       'world'], y= median)
ax.set(xlabel='channels', ylabel='median_shares')
plt.show()

"""We can see that articles belonging to social media recieved highest popularity.

#Feature selection

1. Dropping URL and Time Delta as it has no predictive power
2. Dropping ' weekday_is_saturday', ' weekday_is_sunday' as we have 'is_weekend' column which denotes the same thing.
3. Removing 'kw_min_min',	'kw_max_min', 'kw_min_max',	'kw_max_max','kw_min_avg',	'kw_max_avg' , 'self_reference_min_shares',	'self_reference_max_shares', 'min_positive_polarity',	'max_positive_polarity','min_negative_polarity',	'max_negative_polarity' as we are interested in only average values.

### Shares vs Number of Images
"""

sns.scatterplot(data=data, x=" num_imgs", y=" shares")

"""Shares vs Number of Videos in News post"""

sns.scatterplot(data=data, x=" num_videos", y=" shares")

"""##Correlation Matrices"""

df= data[[' weekday_is_saturday', ' weekday_is_sunday',' is_weekend']]
corrMatrix = df.corr()
sns.heatmap(corrMatrix, annot=True)
plt.show()

df= data[[' n_non_stop_unique_tokens', ' n_unique_tokens']]
corrMatrix = df.corr()
sns.heatmap(corrMatrix, annot=True)
plt.show()

"""### Dropping columns"""

drop= [' weekday_is_saturday', ' weekday_is_sunday', 'url',	' timedelta', ' kw_min_min', ' kw_max_min', ' kw_min_max', ' kw_max_max',' kw_min_avg', ' kw_max_avg' , ' self_reference_min_shares', ' self_reference_max_shares', ' min_positive_polarity', ' max_positive_polarity',' min_negative_polarity', ' max_negative_polarity']

cleaned_1= data.drop(drop,axis=1)

cleaned_1.head()

threshold = 0.6

def high_cor_function(df):
    cor = df.corr()
    corrm = np.corrcoef(df.transpose())
    corr = corrm - np.diagflat(corrm.diagonal())
    print("max corr:",corr.max(), ", min corr: ", corr.min())
    c1 = cor.stack().sort_values(ascending=False).drop_duplicates()
    high_cor = c1[c1.values!=1]    
    thresh = threshold 
    display(high_cor[high_cor>thresh])

high_cor_function(cleaned_1)

plt.figure(figsize=(40,25))
cor = cleaned_1.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

drop2=[' n_non_stop_unique_tokens', ' n_unique_tokens', ' n_non_stop_words', ' LDA_02', ' global_rate_negative_words', ' LDA_00', ' LDA_04', ' global_rate_positive_words', ' rate_positive_words']

cleaned_2= cleaned_1.drop(drop2,axis=1)

cleaned_2.dtypes

cleaned_2.to_csv('Newspop.csv') 
files.download('Newspop.csv')

cleaned_2[' shares'].value_counts()

X=cleaned_2.drop(' shares',axis=1)
y=cleaned_2[' shares']

"""#Linear Regression"""

uploaded = files.upload()
df=pd.read_csv('Newspop.csv')

df=df[0:8000]
df=df.drop('Unnamed: 0',axis=1)

# self is added to make function global
# gd is added to solve by gradient descent
# parameters belong to class when you add self to them
# tolerance specifies the minimal allowed movement in each iteration
# we add bias because column one has zeroes
# Vector is a sequential container to store elements and not index based. Array stores a fixed-size sequential collection of elements of the same type and it is index based. Vector is dynamic in nature so, size increases with insertion of elements. As array is fixed size, once initialized can't be resized.

class LinearRegression:
    def __init__(self, X,y,tolerance,learningrate, maxIteration=50000,error='rmse', gd=False,reg=False,sgd=False): 
        self.X=X
        self.y=y 
        self.learningrate=learningrate
        self.maxIteration=maxIteration
        self.error=error
        self.gd=gd
        self.tolerance = tolerance
        self.reg=reg
        self.sgd=sgd
    
    def trainTestSplit(self,X,y):
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
        
        return X_train, X_test, y_train, y_test
    
    def addX0(self,X):
        """
        @X: numpy matrix,dataset
        @does: add a bias term to the data
        @return: numpy matrix
        """
        return np.column_stack([np.ones([X.shape[0],1]),X])
    
    def normalize(self,X):
        mean= np.mean(X,0) # 0 because we want to find mean of each column
        std= np.std(X,0)
        X_norm= (X-mean)/std
        X_norm= self.addX0(X_norm) # calling a function inside a class as it belong to it
        
        return X_norm, mean, std
    
    def normalizeTestData(self,X,trainMean, trainStd):
        """
        @X: numpy matrix,dataset
        @does: normalize the test data
        @return: numpy matrix
        """
        
        X_norm= (X-trainMean)/ trainStd
        X_norm= self.addX0(X_norm)
        return X_norm
        
    def rank(self,X,eps=1e-12):
        u, S,vh =np.linalg.svd(X)
        return len([x for x in S if abs(x)>eps])
    
    def checkMatrix(self,X):
        x_rank= self.rank(X)
        if x_rank== min(X.shape[0],X.shape[1]):
            self.fullrank=True # We can add a property here it will added automatically
            print('Matrix is full rank')
            
        else:
            self.fullrank=False
            print('Matrix is not full rank')
            
    def checkInvertibility(self,X):
        if X.shape[0]< X.shape[1]:
            self.lowrank=True
            print('The matrix is low rank')
        else:
            self.lowrank=False
            print('Matrix is not low rank')
            
    def closedFormSolution(self, X, y):
        """
        @X: numpy matrix, dataset
        @Y: numpy array, target value
        @does: solve reg using closed form sol
        @ returns numpy array 
        """
        return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
            
    def closedFormSolutionreg(self, X, y):
                             
        """
        @X: numpy matrix, dataset
        @Y: numpy array, target value
        @does: solve reg using closed form sol
        @ returns numpy array 
        """
        return np.linalg.inv(X.T.dot(X)+self.learningrate*np.identity(X.shape[1])).dot(X.T).dot(y)
    
    
    
    def sse(self,X,y):
        y_hat=self.predict(X)
        return ((y_hat-y)**2).sum()
    
    def rmse(self,X,y):
        return math.sqrt(self.sse(X,y)/y.size)
    
    def costFunction(self,X,y):
        return self.sse(X,y) /2
    
    def costDerivative(self,X,y):
        y_hat=self.predict(X)
        return (y_hat -y).dot(X) #2X^TQX - 2 X^Ty (y_hat is QX and X^T is witten as dot(X)
    
    
    def costDerivativereg(self,X,y):
        y_hat=self.predict(X)
        return (y_hat -y).dot(X)+ self.learningrate*self.w 
    
    def gradientDescent(self,X,y):
        error_sequences = []
        last_error=float('inf')
        
        
        for i in tqdm(range(self.maxIteration)):
            if self.reg==False:
                
                self.w=self.w-self.learningrate*self.costDerivative(X,y)
            else:
                 self.w=self.w-self.learningrate*self.costDerivativereg(X,y)
                    
            if self.error=='rmse':
                current_error=self.rmse(X,y)
            else:
                current_error=self.sse(X,y)
            
            diff= last_error-current_error
            last_error=current_error
            
            if diff < self.tolerance:
                print("The model stopped- no further improvement")
                break
                
        return
    
    def SGD(self,X,y,k):
        self.k=k
            
        error_sequences = []
        last_error=float('inf')
            
        temp_X=X[0:50,:]
        temp_y=y[0:50]
        
        for i in tqdm(range(self.maxIteration)):
            if self.reg==False:
                
                self.w=self.w-self.learningrate*self.costDerivative(temp_X,temp_y)
            else:
                self.w=self.w-self.learningrate*self.costDerivativereg(temp_X,temp_y)
                    
            if self.error=='rmse':
                current_error=self.rmse(temp_X,temp_y)
            else:
                current_error=self.sse(temp_X,temp_y)
            
            diff= last_error-current_error
            last_error=current_error
            
            if diff < self.tolerance:
                print("The model stopped- no further improvement")
                break
                
        return
            
    
    def predict(self, X):
        
        return X.dot(self.w) # we can call a function even before defining only in class
        
    
    def trainModel(self): #one function that contails all
        self.X_train,self.X_test,self.y_train,self.y_test=self.trainTestSplit(self.X,self.y)
        

        self.X_train,self.mean,self.std= self.normalize(self.X_train)
        self.X_test=self.normalizeTestData(self.X_test,self.mean,self.std)
        self.checkMatrix(self.X_train)
        self.checkInvertibility(self.X_train)
        
        if self.fullrank and not self.lowrank and self.X_train.shape[0]<10000 and not self.gd:
            if self.reg==False:
                self.w=self.closedFormSolution(self.X_train,self.y_train)
            else:
                self.w=self.closedFormSolutionreg(self.X_train,self.y_train)
        else:
            
            
            if self.sgd==False:
                
                self.w=np.ones(self.X_train.shape[1],dtype=np.float64)*0
                self.gradientDescent(self.X_train,self.y_train)
            
            else:
                
                self.w=np.ones(self.X_train.shape[1],dtype=np.float64)*0
                self.SGD(self.X_train,self.y_train,12)
            
        
        print(self.w)
        
        if self.error=='rmse':
            print(self.rmse(self.X_test,self.y_test))
        else:
            print(self.sse(self.X_test,self.y_test))

regression= LinearRegression(df.values[:,0:-1],df.values[:,-1],learningrate=0.000001,tolerance=0.000000001,gd= True, error= 'rmse',reg=True,sgd=False)
regression.trainModel()

"""#Logistic Regression"""

cleaned_3=cleaned_2
cleaned_3.head()

# using median value to make it a classification problem
rows = len(cleaned_3)

for i in range(0,rows):
  value = cleaned_3[' shares'][i]
  
  if value >= 1400:
    cleaned_3[' shares'][i] = 1

  else:
    cleaned_3[' shares'][i] = 0

cleaned_3.head()

cleaned_3[' shares'].value_counts()

X=cleaned_3.drop(' shares',axis=1)
y=cleaned_3[' shares']

X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=0.2, random_state=123)
y_train.head()

columns= [' n_tokens_title',	' n_tokens_content',	' num_hrefs',	' num_self_hrefs',	' num_imgs', ' num_videos', ' average_token_length'	, ' num_keywords', ' kw_avg_min',	' kw_avg_max',' kw_avg_avg',' self_reference_avg_sharess',' LDA_01',	' LDA_03',	' global_subjectivity',	' global_sentiment_polarity'	,' rate_negative_words',	' avg_positive_polarity',	' avg_negative_polarity',	' title_subjectivity',	' title_sentiment_polarity',	' abs_title_subjectivity',	' abs_title_sentiment_polarity']

scaler = StandardScaler()

X_train[columns] = scaler.fit_transform(X_train[columns])

X_val[columns]=scaler.transform(X_val[columns])



X_train.head()

train_df= pd.concat([X_train,y_train],axis=1)
test_df= pd.concat([X_val,y_val],axis=1)
train_df[' shares'].value_counts()

"""### Logistic Regression Class Definition"""

class LogisticRegression:
    def __init__(self,learningRate, tolerance,maxIteration=50000):
        self.learningRate=learningRate
        self.tolerance=tolerance
        self.maxIteration=maxIteration
        
    def dataset_reader (self):
        global X_train,y_train;
        train_df= pd.concat([X_train,y_train],axis=1)
        test_df= pd.concat([X_val,y_val],axis=1)
      
        train_df=np.array(train_df, dtype=np.float128) #converting dataframe to matrix
        test_df=np.array(test_df, dtype=np.float128)
        col=X_train.shape[1]
        colt=X_val.shape[1]
        X_train,y_train=train_df[:,:col-1], train_df[:,-1] 
        X_test,y_test=test_df[:,:colt-1], test_df[:,-1]
        
        return X_train,X_test, y_train,y_test
    
    # adding bias
    
    def add_x0(self , X):
        return np.column_stack([np.ones(X.shape[0],1),X])
        
        
        
    def sigmoid (self,z):
        sig=1/(1+ np.exp(-z))
        return sig 
    
    def cost_function(self,X,y):
        
        sig=self.sigmoid(X.dot(self.w))
        #pred=y*np.log(sig)+ (1-y) * np.log(1-sig)
        #cost=pred.sum() #output is vector
        
        # or:
        
        pred_= np.log(np.ones(X.shape[0])+ np.exp(sig))- X.dot(self.w).dot(y)
        cost=pred_.sum()
        
        return cost
    
    def gradient(self,X,y):
        
        sig=self.sigmoid(X.dot(self.w))
        grad=(sig-y).dot(X)
        
        return grad
    
    def gradient_descent(self,X,y):
        
        cost_sequence= [] #for each iteration add that cost to the vector
        
        last_cost= float('inf')
        tolerance_counter=0
        
        # print(self.maxIteration, 'max_iteration')
        for i in tqdm(range(self.maxIteration)):
            self.w = self.w - self.learningRate * self.gradient(X,y)
            current_cost=self.cost_function(X,y)
            diff=  last_cost - current_cost
            last_cost = current_cost
            cost_sequence.append(current_cost)
            
            if diff < self.tolerance:
                tolerance_counter += 1
                print('The model stopped - no further improvement')
                
                #if tolerance_counter == 10:
                break
                    
            
        
        self.plot_cost(cost_sequence)
        #return
    
    
    def plot_cost(self,cost_sequence):
        s=np.array(cost_sequence)
        t=np.arange(s.size)
        
        fig,ax=plt.subplots()
        ax.plot(t, s)
        ax.set(xlabel='iterations',ylabel='cost',title='cost trend')
        
        ax.grid()
        plt.legend(bbox_to_anchor=(1.05,1),loc=2, shadow=True)
        plt.show()
        
    
    def predict(self,X):
        
        sig=self.sigmoid(X.dot(self.w))
        return np.around(sig)
    
    def evaluate(self,y,y_hat):
        y=(y==1)
        y_hat=(y_hat==1)
        
        accuracy= (y == y_hat).sum() / y.size
        precision = (y&y_hat).sum() / y_hat.sum()
        recall=(y & y_hat).sum()/y.sum()
        

        return accuracy, recall, precision

               
    def run_model(self):
        
        # reading the data 
        
        self.X_train, self.X_test, self.y_train, self.y_test=self.dataset_reader()

        # print('hereeeeee',self.X_train)
    

        #initiate w
        
        self.w= np.ones(self.X_train.shape[1],dtype= np.float64)*0
        
        self.gradient_descent(self.X_train,self.y_train)
        
        # print(self.w)
        
        y_hat=self.predict(self.X_train)
        # print('here1')
        # print(y_hat)
        accuracy, recall, precision= self.evaluate(self.y_train,y_hat)

        
        #Comparing Accuracy
        
        print('Accuracy', accuracy)
        print('Recall',recall)
        print('precision', precision)

lr=LogisticRegression(tolerance=0.0, learningRate=0.1e-5,maxIteration=1000)

lr.run_model()

"""#Support Vector Machine

###Support Vector Machine Class Definition
"""

class SupportVectorMachine:
  def __init__(self, learning_rate=0.001, lambda_param = 0.01, n_iterations=100):
    self.learning_rate = learning_rate
    self.lambda_param = lambda_param
    self.n_iterations = n_iterations
    self.w = None
    self.b = None
  
  def fit(self, X, y):
    n_samples, n_features = X.shape
    self.w = np.zeros(n_features)
    self.b = 0

    for i in range(self.n_iterations):
      for idx, x_i in enumerate(X):
        condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1
        if condition:
          self.w -= self.learning_rate * (2 * self.lambda_param * self.w)
        else:
          self.w -= self.learning_rate * (2 * self.lambda_param * self.w - np.dot(x_i, y[idx]))
          self.b -= self.learning_rate * y[idx]
  
  def predict(self, X):
    output = np.sign(np.dot(X, self.w) - self.b)

    return output

"""### Loading Data"""

def load_data():
    # df = pd.read_csv("OnlineNewsPopularity.csv")
    df = data.dropna()
    df = df.iloc[:, 2:]
    X = df.iloc[:, :-1]

    X=((X-X.min())/(X.max()-X.min())).values
    y = df[' shares'].values
    
    y[y < 1400] = -1
    y[y >= 1400] = 1

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
    
    return X_train, X_test, y_train, y_test

X_train_SVM, X_test_SVM, y_train_SVM, y_test_SVM = load_data()

"""### Initialize and fit model"""

svm = SupportVectorMachine()
svm.fit(X_train_SVM, y_train_SVM)

"""### Making prediction"""

y_pred_SVM = svm.predict(X_test_SVM)
loss = log_loss(y_test_SVM, y_pred_SVM)

print('Predicted class output: ', y_pred_SVM)
print('Log Loss for SVM model: ', loss)

accuracy_SVM = accuracy_score(y_test_SVM, y_pred_SVM)
print("Accuracy with SVM: ", accuracy_SVM)

recall_SVM = recall_score(y_test_SVM, y_pred_SVM)
print("Recall with SVM: ", recall_SVM)

"""# Neural Network MLP Classifier

###Define supporting functions
"""

def random_mini_batches(X, Y, mini_batch_size = 64):
        """
        @X: input data, of shape (input size, number of examples)
        @Y: true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)
        @mini_batch_size: size of the mini-batches, integer
        
        @does: Creates a list of random minibatches from (X, Y)
        @returns: mini_batches - list of synchronous (mini_batch_X, mini_batch_Y)
        """
        
        m = X.shape[1]                  # number of training examples
        mini_batches = []
        
        # shuffle the rows of the dataset in random manner for both X and y
        permutation = list(np.random.permutation(m))
        shuffled_X = X[:, permutation]
        shuffled_Y = Y[permutation]
       
        # make partitions from the shuffled data
        num_complete_minibatches = math.floor(m/mini_batch_size) 
        for k in range(0, num_complete_minibatches):
            mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]
    
            mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size]
    
            mini_batch = (mini_batch_X, mini_batch_Y)
            mini_batches.append(mini_batch)
    
        # to handle the case when the mini batch size is less than preset mini_batch_size
        if m % mini_batch_size != 0:
            mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]
            mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m]
            mini_batch = (mini_batch_X, mini_batch_Y)
            mini_batches.append(mini_batch)
        
        return mini_batches

"""### Neural Network Class Definition"""

class NeuralNetworkClassifier:
    def __init__(self, learning_rate = 0.000001):
        self.learning_rate = learning_rate
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)
    
    def initialize_parameters(self):
        """
        @does: initializes weight and bias parameters for the Neural Network 
        """
    
        initializer = tf.initializers.GlorotUniform(seed=1)
        w1 = tf.Variable(initializer(shape = (32, 58)), name='w1')
        b1 = tf.Variable(tf.zeros([32,1]), name = 'b1')
        w2 = tf.Variable(initializer(shape = (16, 32)), name='w2')
        b2 = tf.Variable(tf.zeros([16,1]), name = 'b2')
        w3 = tf.Variable(initializer(shape = (1, 16)), name='w2')
        b3 = tf.Variable(tf.zeros([1,1]), name = 'b2')
        
        
        self.parameters = {"w1":w1,
                        "b1":b1,
                        "w2":w2,
                        "b2":b2,
                        "w3":w3,
                        "b3":b3}
    
    
    def feedForward(self, X):
        """
        @X: numpy matrix, dataset
        @does: propogates data through NN layers using updated parameters, ends at z3 - NN logits
        """
        
        z1 = tf.add(tf.matmul(self.parameters['w1'], X), self.parameters['b1'])
        a1 = tf.nn.relu(z1)
        z2 = tf.add(tf.matmul(self.parameters['w2'], a1), self.parameters['b2'])
        a2 = tf.nn.relu(z2)
        self.z3 = tf.add(tf.matmul(self.parameters['w3'], a2), self.parameters['b3'])
    
    
    def compute_cost(self, y):
        """
        @y: numpy array, target variable
        @does: computes cost used for gradient calculation
        """
        
        logits = tf.transpose(self.z3)
        labels = y
        
        # sigmoid cross entropy good option for binary classification problem
        self.cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))
        
    
    def get_grad(self, X, y):
        """
        @X: numpy matrix, independent variable - mini batch
        @y: numpy array, target variable - mini batch
        @does: calculates the gradients for all nodes w.r.t the cost - used for optimization of parameters
        """
        
        with tf.GradientTape() as tape:
            # get logit output for the NN
            self.feedForward(X)
            # evaluate the cost for the computed set of logit values compared to the label for the mini batch
            self.compute_cost(y)
            # compute the gradient w.r.t each node using parameter values
            self.grads = [tape.gradient(self.cost, list(self.parameters.values())), self.cost]
    
    
    def fitNN(self, X_train, y_train, num_epochs = 10, minibatchSize = 50):
        """
        @X_train: numpy matrix, training set - user input
        @y_train: numpy array, training target - user input
        @num_epochs: user can define number of epochs NN should run through
        @minibatchSize: size of mini training batch 
        @does: fits the NN model on the dataset and optimizes the parameters
        """

        m = X_train.shape[1]
        self.initialize_parameters()        
        
        for epoch in range(num_epochs):
            
            epoch_cost = 0
            num_minibatches = (m/minibatchSize)
            minibatches = random_mini_batches(X_train, y_train, mini_batch_size=minibatchSize)
            
            for minibatch in minibatches:
                (minibatch_x, minibatch_y) = minibatch
                
                x = minibatch_x.astype(np.float32)
                y = minibatch_y.astype(np.float32)
                
                self.get_grad(x, y)
                self.optimizer.apply_gradients(zip(self.grads[0], list(self.parameters.values())))
                epoch_cost += self.grads[1] / num_minibatches
                
            print("Cost after epoch %i: %f"% (epoch, epoch_cost))
            
    def predictNN(self, X, y):
        """
        @X: numpy matrix, testing dataset - user input
        @y: numpy matrix, testing target - user input
        @does: makes predictions based on parameters determined by the fitNN method
        @return:
        @pred: tensor of prediction
        @binary_cross_entropy: binary cross entropy error computed using sigmoid output
        """

        z1 = tf.add(tf.matmul(self.parameters['w1'], X), self.parameters['b1'])
        a1 = tf.nn.relu(z1)
        z2 = tf.add(tf.matmul(self.parameters['w2'], a1), self.parameters['b2'])
        a2 = tf.nn.relu(z2)
        z3 = tf.add(tf.matmul(self.parameters['w3'], a2), self.parameters['b3'])
        a3 = tf.nn.sigmoid(z3)
        
        pred = tf.round(a3)
        bce = tf.keras.losses.BinaryCrossentropy()
        binary_cross_entropy = bce(y, tf.transpose(a3)).numpy()
        
        return {'prediction':pred, 'BCE_Loss': binary_cross_entropy}

"""###Transform original dataset as required by Neural Network"""

def load_data():
    # df = pd.read_csv("OnlineNewsPopularity.csv")
    df = data.dropna()
    df = df.iloc[:, 2:]
    X = df.iloc[:, :-1]

    X=((X-X.min())/(X.max()-X.min())).values
    y = df.iloc[:, -1:].values
    
    y[y < 1400] = 1
    y[y >= 1400] = 0
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
    
    X_train = X_train.reshape(X_train.shape[0], -1).T
    X_test = X_test.reshape(X_test.shape[0], -1).T
    
    return X_train, X_test, y_train, y_test

X_train_NN, X_test_NN, y_train_NN, y_test_NN = load_data()

"""###Load Data, Initilize Class, Train NN model"""

nn = NeuralNetworkClassifier()
nn.fitNN(X_train_NN, y_train_NN, num_epochs = 100)

"""### Make Prediction using Test Dataset"""

pred = nn.predictNN(X_test_NN, y_test_NN)
y_pred_NN = pred['prediction'].numpy()
binary_cross_entropy_NN = pred['BCE_Loss']
print("Predicted Values:", y_pred_NN)
print("Binary Cross Entropy error:", binary_cross_entropy)

acc = tf.keras.metrics.Accuracy()
acc.update_state(y_test_NN.T, y_pred_NN)
accuracy_NN = acc.result().numpy()

print("Accuracy from NN: ", accuracy_NN)


rec = tf.keras.metrics.Recall()
rec.update_state(y_test_NN.T, y_pred_NN)
recall_NN = rec.result().numpy()

print("Recall from NN: ", recall_NN)